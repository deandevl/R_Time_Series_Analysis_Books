---
title: "An Introduction to Forecasting"
output: 
  html_document:
    toc: yes
    toc_depth: 3
    css: ../style.css
params:
  date: !r Sys.Date()        
---

```{r, message = FALSE, warning = FALSE}
library(data.table)
library(tseries)
library(forecast)
library(astsa)
library(ggplot2)
library(grid)
library(gtable)
library(RtsaPkg)
library(RplotterPkg)
```

```{r,setup, include=FALSE, eval=TRUE}
options(knitr.table.format = "html", width = 140)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 12, fig.height = 8)
```

<div>Author: Rick Dean</div>
<div>Article date: `r params$date`</div>

<div class="abstract">
  <p class="abstract">Abstract</p>
The following notes/scripts/plots are inspired by an excellent article [An Introduction to Forecasting](https://www.datascienceblog.net/post/machine-learning/forecasting-an-introduction/) by [Matthias DÃ¶ring](https://www.datascienceblog.net/about-data-science-with-r/)
</div>


# Important Concepts

## The backshift operator
Given the time series $y = \lbrace y_1, y_2,...\rbrace$, the backshift operator (also called the lag operator) is
defined as:

$$B_{y_t} = y_{t-1}, \forall t > 1$$.

One application of the backshift operator yields the previous measurement in the time series.  Raising
the backshift operator to a power *k* > 0 performs multiple shifts at once:

$$B^ky_t = y_{t-k}$$
$$B^{-k}y_t = y_{t+k}$$

For example $B^2y_t$ yields the measurement that was observed two time periods earlier.  Instead of $B$, $L$ is
used equivalently to indicate the lag operator.

### Lagged differences with the backshift operator
The backshift operator can be used to calculate lagged differences for a time series of values via
$y_i - B^k(y_i), \forall i \in k + 1,...,t$ where *k* indicates the lag of the differences.  For *k* = 1
we obtain ordinary pairwise differences.

Using the `base::diff()` function to calculate lagged differences. The second arg sets the desired lag.
Default is 1.
```{r}
y <- c(1,3,5,10,20)
(By_1 <- diff(y))
(By_3 <- diff(y,3))
```
## The autocorrelation function
Defines the correlation of a variable $y_t$ to previous measurements $y_{t-1}, \dotsb y_1$ of
the same variable (hence the name autocorrelation).

The autocorrelation for lag *k* is defined as:
$$\varphi_k := Corr(y_t, y_{t-k}) k = 0,1,2,\dotsb$$
A function that constructs two vectors, $y_t$ and $y_{t-k}$ according
to the *lag* argument:

```{r}
autocor <- function(x,lag=1){       # [1, 3,  5, 10, 20]
  x.left <- x[1:(length(x) - lag)]  # [3, 5, 10, 20]
  x.right <- x[(1+lag):(length(x))] # [1, 3,  5, 10]
  val <- cor(x.left, x.right)
  return(val)
}
(autocor(y))
(autocor(y,2))
```
## Partial autocorrelations
Note that the autocorrelation (ACF) function does not control for the other lags.  The partial
autocorrelation (pACF) does regress the values of the time series at all shorter lags.

The partial autocorrelation (pACF) at lag k is the correlation that results after removing
the effect of any correlations due to the terms at shorter lags. Given a time series z<sub>t</sub>,
the pACF is the autocorrelation between z<sub>t</sub> and z<sub>t+k</sub> with the linear dependence
of z<sub>t</sub> on z<sub>t+1</sub> and z<sub>t+k-1</sub> removed.

The partial autocorrelation of an AR(k) process is zero at lags k + 1 and greater.  To help
determine the order of an observed process, one looks at the point on the plot where the partial
autocorrelation for all higher lags are essentially zero.

An approximate test that a given partial correlation is zero (at a 5% significance level) is given
by comparing the sample partial autocorrelation against the critical region with the upper and lower
limits given by +-1.96/sqrt(N), where N is the record length of the time series.  This approximation
relies on the assumption that the record length is at least moderately large (N>30) and that the
underlying process has a finite second moment.


```{r, fig.height = 10, fig.width = 8}
y_df <- data.frame(
  time = 1:11,
  series = c(1,3,5,10,20,25,30,35,40,45,50)
)
y_acf <- RtsaPkg::graph_acf(
  df = y_df,
  time_col = "time",
  value_col = "series",
  title = "ACF and pACF of a simple 11 point series",
  max_lag = 3,
  bold_y = 0.0,
  confid_level = 1.96,
  ac_y_limits = c(-0.4, 1),
  ac_y_major_breaks = seq(-0.4,1,0.2),
  pac_y_limits = c(-0.4,0.4),
  pac_y_major_breaks = seq(-0.4,0.4,0.2),
  show_minor_grids = FALSE
)
```

## Decomposing time-series data
Asking whether the time series data are additive or multiplicative.  The main difference between additive and
multiplicative time series is the following:

1. Additive: amplitudes of seasonal effects are similar in each period
2. Multiplicative: seasonal trend changes with the progression of the time series

### Multiplicative time series `datasets::AirPassengers` data set.
```{r, fig.height = 8, fig.width = 10}
airpass_dt <- RtsaPkg::ts_to_df(datasets::AirPassengers)
RplotterPkg::create_scatter_plot(
    df = airpass_dt,
    aes_x = "DateTime",
    aes_y = "V1",
    title = "Box & Jenkins Airline Data",
    subtitle = "Monthly totals of international passengers 1949 - 1960",
    x_title = "Month",
    y_title = "Air Passengers",
    connect = TRUE,
    rot_y_tic_label = TRUE,
    x_major_breaks = seq(as.Date("1949-01-01"), as.Date("1960-12-01"), by = "1 year"),
    x_date_labels = "%Y",
    y_major_breaks = seq(100,600,50),
    show_minor_grids = FALSE
)
```

Note that the amplitude of the seasonal trend is increasing through the years. To adjuse for this effect, we have to take the logarithm of the measurements.  The logarithm turns the multiplicative model into an additive: $log(S_iT_t\epsilon_t) = log(S_t) + log(T_t) + log(\epsilon_t)$.

### Plotting the measurements on a log10 scale
```{r, fig.height = 8, fig.width = 10}
RplotterPkg::create_scatter_plot(
    df = airpass_dt,
    aes_x = "DateTime",
    aes_y = "V1",
    title = "Box & Jenkins Airline Data",
    subtitle = "Monthly totals of international passengers 1949 - 1960",
    x_title = "Month",
    y_title = "log(Air Passengers)",
    y_log10 = TRUE,
    connect = TRUE,
    rot_y_tic_label = TRUE,
    x_major_breaks = seq(as.Date("1949-01-01"), as.Date("1960-12-01"), by = "1 year"),
    x_date_labels = "%Y",
    y_major_breaks = seq(100,600,50),
    show_minor_grids = FALSE
)
```

The logarithm scale has equalized the amplitude of the seasonal component along time.  Note that the overall trend has not changed.

### Using multiplicative decomposition 
```{r, fig.height = 11, fig.width = 10}
airpass_decompose <- RtsaPkg::graph_decompose(
  series_ts = datasets::AirPassengers,
  type_comp = "multiplicative",
  title = "Decompose Box & Jenkins Airline Data",
  subtitle = "Monthly totals of international passengers 1949 - 1960",
  x_title = "Year",
  y_title = "Passengers",
  x_major_breaks = seq(as.Date("1949-01-01"), as.Date("1960-12-01"), by = "1 year"),
  x_date_labels = "%Y",
  show_minor_grids = FALSE
)
```
<div class="note">There appears to be an increasing trend and the seasonal component is consistent.</div>

### Using additive decomposition
1. `datasets::EuStockMarkets` is a multiple time series object (mts) so select the German series.
```{r}
german_ts <- datasets::EuStockMarkets[,1]
str(german_ts)
```
2. Decompose the time series object using `RtsaPkg::graph_decompose()`.
Note that "additive" is the default type decomposition.
```{r, fig.height = 11, fig.width = 10}
german_decompose <- RtsaPkg::graph_decompose(
    series_ts = german_ts,
    title = "Decompose German Stock Market Daily Closing Prices",
    subtitle = "1991 - 1998",
    x_title = "Year",
    y_title = "Price",
    x_major_breaks = seq(as.Date("1991-01-01"), as.Date("1998-12-01"), by = "1 year"),
    x_date_labels = "%Y",
    show_minor_grids = FALSE
)
```
<div class="note">Shows an overall increasing trend with a seasonal peak in the end of summer. The random noise hovers around zero except toward the end of the series.</div>

## Stationary vs. non-stationary processes
A process is stationary if its mean and variance are not shifting along the time line.  Both `datasets::AirPassengers` and `datasets::EuStockMarkets` are non-stationary because they both have increasing trends throughout the time line.

### A stationary process
1. View the structure of `nino3.4` series:
```{r}
data(nino)
str(nino3.4)
```
2. Convert the `nino3.4` series to a data.frame:
```{r}
nino3.4_dt <- RtsaPkg::ts_to_df(ts_obj = nino3.4)
str(nino3.4_dt)
```
3. Plot the `nino3.4` series:
```{r, fig.height = 8, fig.width = 13}
RplotterPkg::create_scatter_plot(
    df = nino3.4_dt,
    aes_x = "DateTime",
    aes_y = "V1",
    title = "Nino Region 3.4 SST Monthly Temperatures (deg C)",
    subtitle = "1950 - 2000",
    x_title = "Year",
    y_title = "Temperature",
    rot_y_tic_label = TRUE,
    x_major_breaks = seq(as.Date("1950-01-01"), as.Date("2000-12-01"), by = "2 year"),
    x_date_labels = "%Y",
    show_pts = FALSE,
    connect = TRUE
)
```
<div class="note">Very little trend across the time line.</div>

# The ARMA model
ARMA stands for autoregressive moving average. ARMA models are only appropriate for stationary processes and have two parameters:

* $p$: the order of the autoregressive (AR) model
* $q$ the order of the moving average (MA) model

The ARMA model can be specified as:
$$ \hat{y}_t = c + \epsilon_t + \sum_{i=1}^p\phi_iy_{t-i} - \sum_{j=1}^q\theta_j\epsilon_{t-j} $$
with the following variables:

* $c$: the intercept of the model (e.g. the mean)
* $\epsilon_t$: random error (white noise, residual) associated with measurement *t* with $\epsilon_t \sim N(0,\sigma)$.
* $\phi \in \mathbb{R}^p$: a vector of coefficients for the AR terms. In R, these parameters are called *AR1*, *AR2*, etc.
* $y_t$: outcome measured at time *t*.
* $\theta \in \mathbb{R}^q$: a vector of coefficients for the MA terms. In R, these parameters are called *MA1*, *MA2*, etc.
* $\epsilon_t$: noise associated with measurement *t*.

## Formulating the ARMA model using the backshift operator
Using the backshift operator, we can formulate the ARMA model in the following way:
$$(1 - \sum_{i=1}^p\phi_iB^i)y_t = (1 - \sum_{j=1}^q\theta_jB^j)\epsilon_j$$

By defining $\phi_p(B)$ = $1 - \sum_{i=1}^p\phi_iB^i$ and $\theta_q(B)$ = $1 - \sum_{j=1}^q\theta_jB^j$, the ARMA model
simplifies to;
$$\phi_p(B)y_t = \theta_q(B)\epsilon_t $$.

# The ARIMA model
## Concepts
ARIMA stands for autoregressive integrated moving average and is a generalization of the ARMA model.  In contrast to ARMA models
ARIMA models are capable of dealing with non-stationary data, that is, time-series where the mean and variance changes over time.
This feature is indicated by the (integrated) of ARIMA: an initial differencing step can eliminate the non-stationarity.  For
this purpose ARIMA require an additional parameter, d.  Taken together an ARIMA model has the following three parameters:

* $p$: the order of the autoregressive (AR) model
* $d$: the degree of differencing
* $q$: the order of the moving average (MA) model

In the ARIMA model, outcomes are transformed to differences by replacing $y_t$ with differences of the form:
$$(1 - B)^dy_t$$
The model is then specified by
$$\phi_p(B)(1 - B)^dy_t = \theta_q(B)\epsilon_t$$

For $d = 0$ the model simplifies to the ARMA model since $(1 - B)^0y_t = y_t$. For other choices of $d$ we obtain backshift ploynomials, for
example:

$$(1 - B)^1y_t = y_t - y_{t-1}$$
$$(1 - B)^2y_t = (1 -2B + B^2)y_t = y_t - 2y_{t-1} + y_{t-2}$$
In the following let us consider the interpretation of the three parameters of ARIMA models.

### ARIMA model and $p$
The parameter $p\in \mathbb{N}_0$ specifies the order of the autoregressive model. The term *order* refers to the number of lagged
differences that the model considers. For simplicity let us assume that $d = 0$ (no differencing). Then an AR model of order 1
considers only the most recent measurements, that is, $By_t = y_{t-1}$ via the parameter $\phi_1$.  An AR model of order 2, on the
other hand would consider the last two points in time, that is, measurements $y_{t-1}$ as well as $y_{t-2}$ through $\phi_1$ and
$\phi_2$, respectively.

The number of autoregressive terms indicates the extent to which previous measurements influence the current outcome. For example,
ARIMA(1,0,0), which has $p = 1$, $d = 0$ and $q = 0$, has an autoregressive term of order 1, which means that the outcome is
influenced only by the most recent previous measurements. In this case the model simplifies to
$$\hat{y}_t = \mu\epsilon_t + \phi_1y_{t-1}$$

Question: How do we get this simplication with the product of $\mu\epsilon_t$?

### Impact of ARIMA model
We can simulate autoregressive processes using the  `stats::arima.sim()` function. Via the function the
model can be specified by providing the coefficients for the MA and AR terms to be used. In the following we will plot
the autocorrelation, because it is best suited for finding the impact of autoregression.

### Simulate ARIMA(1,0,0) autoregression series
1. Simulate an autoregresssive series and convert the resulting time object to a data.frame:
```{r}
set.seed(5)
ar_1 <- arima.sim(list(ar = 0.75), n = 1000)
str(ar_1)
ar_1_dt <- RtsaPkg::ts_to_df(ts_obj = ar_1)
```
2. Plot ar_1:
```{r, fig.height = 10, fig.width = 13}
ar_1_ar <- RtsaPkg::graph_acf(
  df = ar_1_dt,
  time_col = "DateTime",
  value_col = "V1",
  max_lag = 30,
  confid_level = 1.96,
  title = "Autocorrelations of Simulated ARIMA(1,0,0) Series",
  x_title = "Index",
  ac_x_limits = c(1,30),
  ac_x_major_breaks = seq(1,30,1),
  ac_y_limits = c(-0.2, .8),
  pac_y_limits = c(-0.2, .8),
  bold_y = 0.0,
  show_minor_grids = FALSE,
  col_width = 12
)
```
<div class="note">Shows a very high correlation at lag 1.</div>

### Simulate a second order autoregressive process ARIMA(2,0,0)
```{r, fig.height = 10, fig.width = 13}
ar_2 <- arima.sim(list(ar = c(0.65, 0.3)), n = 1000)
ar_2_df <- RtsaPkg::ts_to_df(ts_obj = ar_2)
ar_2_ar <- RtsaPkg::graph_acf(
  df = ar_2_df,
  time_col = "DateTime",
  value_col = "V1",
  max_lag = 30,
  confid_level = 1.96,
  title = "Autocorrelations of Simulated ARIMA(2,0,0) Series",
  x_title = "Index",
  ac_x_limits = c(1,30),
  ac_x_major_breaks = seq(1,30,1),
  ac_y_limits = c(-0.2, 1.0),
  pac_y_limits = c(-0.2, 1.0),
  bold_y = 0.0,
  show_minor_grids = FALSE,
  col_width = 12
)
```
<div class="note">Very high correlations at lags 1 and 2.</div>

The order of the AR term can be selected according to the largest lag at which the pACF was significant.

## ARIMA model and $d$ (degree of differencing)
The parameter $d \in \mathbb{N}_0$ specifies the degree of differencing in the model term $(1 - B)^dy_t$
In practice $d$ should be chosen such that we obtain a stationary process.  An ARIMA(0,1,0) model simplifies
to the random walk model
$$\hat{y}_t = \mu + \epsilon + y_{t-1}$$
The model is random because for every point in time $t$ the mean is simply adjusted by $y_{t-1}$, which leads to random changes of $y_t$ over time.

### Impact of differencing
1. Change `datasets::AirPassengers` to a data.frame and plot the first order differences using `RtsaPkg::graph_dif()`:
```{r, fig.width=12, fig.height=10}
AirPassengers_df <-  RtsaPkg::ts_to_df(ts_obj = datasets::AirPassengers)
AirPassengers_dif <- RtsaPkg::graph_dif(
  df = AirPassengers_df,
  time_col = "DateTime",
  value_col = "V1",
  title = "AirPassengers Monthly Totals First Order Differences",
  x_title = "Year",
  y_title = "Passengers"
)
```
<div class="note">The *AirPassengers* observations are seasonal with increasing trend while by taking differences of the series the lagged differences are stationary.</div>

## ARIMA model and $q$
The moving average model is specified via $q\in\mathbb{N}_0$. The MA term models the past error, $\epsilon_t$ using coefficients $\theta$.
An ARIMA(0,0,1) model simplifies to:
$$\hat{y}_t = \phi + \epsilon_t + \theta_1\epsilon_{t-1}$$
in which the current estimate depends on the residual of the previous measurement.

### Impact of the moving average
1. Show the impact of the moving average by simulating and plotting a ARIMA(0,0,1) process:
```{r, fig.width=13, fig.height=10}
ma_1 <- arima.sim(list(ma = 0.75), n = 1000)
ma_1_df <- RtsaPkg::ts_to_df(ts_obj = ma_1)
ma_1_ma <- RtsaPkg::graph_acf(
  df = ma_1_df,
  time_col = "DateTime",
  value_col = "V1",
  max_lag = 30,
  title = "Simulated Moving Average Process ARIMA(0,0,1)",
  x_title = "Index",
  confid_level = 1.96,
  ac_x_limits = c(1,30),
  ac_x_major_breaks = seq(1,30,1),
  ac_y_limits = c(-0.2, 0.6),
  pac_y_limits = c(-0.4, 0.6),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  col_width = 12
)
```
2. Show the impact of the moving average by simulating and plotting a ARIMA(0,0,2) process:
```{r,fig.width=13,fig.height=10}
ma_2 <- arima.sim(list(ma = c(0.65, 0.3)), n = 1000)
ma_2_df <- RtsaPkg::ts_to_df(ts_obj = ma_2)
ma_2_ma <- RtsaPkg::graph_acf(
  df = ma_2_df,
  time_col = "DateTime",
  value_col = "V1",
  max_lag = 30,
  title = "Simulated Moving Average Process ARIMA(0,0,2)",
  x_title = "Index",
  confid_level = 1.96,
  ac_x_limits = c(1,30),
  ac_x_major_breaks = seq(1,30,1),
  ac_y_limits = c(-0.2, 0.6),
  pac_y_limits = c(-0.4, 0.6),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  col_width = 12
)
```
<div class="note">The first two lags appear to be significant.</div>

## Choosing between AR and MA terms
We need to consider both the ACF and PACF. Using these plots we can differentiate two signatures:

* **AR signature**: The PACF of the differenced time series displays a sharp cutoff or the value at lag 1 in the PACF is positive.
* **MA signature**: The ACF of the differenced time series displays a sharp cutoff. Commonly associated with a negative autocorrelation at lag 1 in the ACF of the differenced series.

### Impact of AR and MA terms together
```{r, fig.width = 13, fig.height = 14}
# ARIMA(1,0,1)
ar_ma_1 <- arima.sim(list(order = c(1,0,1), ar = 0.8, ma = 0.8), n = 1000)
ar_ma_1_df <- RtsaPkg::ts_to_df(ts_obj = ar_ma_1)
ar_ma_1_arma <- RtsaPkg::graph_acf(
  df = ar_ma_1_df,
  time_col = "DateTime",
  value_col = "V1",
  max_lag = 30,
  title = "Simulated Process ARIMA(1,0,1)",
  x_title = "Index",
  confid_level = 1.96,
  layout = "hor",
  ac_x_major_breaks = seq(0,30,5),
  ac_y_limits = c(-0.2,1),
  pac_x_major_breaks = seq(0,30,5),
  pac_y_limits = c(-0.6,1.0),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  col_width = 4,
  display_plot = FALSE
)

# ARIMA(2,0,1)
ar_ma_2 <- arima.sim(list(order = c(2,0,1), ar = c(0.6,0.3), ma = 0.8), n = 1000)
ar_ma_2_df <- RtsaPkg::ts_to_df(ts_obj = ar_ma_2)
ar_ma_2_arma <- RtsaPkg::graph_acf(
  df = ar_ma_2_df,
  time_col = "DateTime",
  value_col = "V1",
  max_lag = 30,
  title = "Simulated Process ARIMA(2,0,1)",
  x_title = "Index",
  confid_level = 1.96,
  layout = "hor",
  ac_x_major_breaks = seq(0,30,5),
  ac_y_limits = c(-0.2,1),
  pac_x_major_breaks = seq(0,30,5),
  pac_y_limits = c(-0.4,1.0),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  col_width = 4,
  display_plot = FALSE
)

# ARIMA(2,0,2)
ar_ma_3 <- arima.sim(list(order = c(2,0,2), ar = c(0.6,0.3), ma = c(0.6,0.3)), n = 1000)
ar_ma_3_df <- RtsaPkg::ts_to_df(ts_obj = ar_ma_3)
ar_ma_3_arma <- RtsaPkg::graph_acf(
  df = ar_ma_3_df,
  time_col = "DateTime",
  value_col = "V1",
  max_lag = 30,
  title = "Simulated Process ARIMA(2,0,2)",
  x_title = "Index",
  confid_level = 1.96,
  layout = "hor",
  ac_x_major_breaks = seq(0,30,5),
  ac_y_limits = c(-0.2,1),
  pac_x_major_breaks = seq(0,30,5),
  pac_y_limits = c(-0.4,1.0),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  col_width = 4,
  display_plot = FALSE
)

layout <- list(
  plots = list(ar_ma_1_arma$plots,ar_ma_2_arma$plots,ar_ma_3_arma$plots),
  rows = c(1,2,3),
  cols = c(1,1,1)
)
multi_plot <- RplotterPkg::multi_panel_grid(
  layout = layout,
  col_widths = 12,
  row_heights = rep(4, 3)
)
```

## The expanded seasonal ARIMA -- the SARIMA Model

To model seasonal trends, we need to expand the ARIMA model with the seasonal parameters $P$, $D$, and $Q$ which correspond to $p$, $d$, $q$ in the original model.

* $P$: number of seasonal autoregressive (SAR) terms
* $D$: degree of seasonal (differencing)
* $Q$: number of seasonal moving average (SMA) terms

The additional parameters are included into the ARIMA model in the following way:
$$\Phi_P(B^S)\phi_P(B)(1 - B)^d(1 - B^S)y_t = \Theta_Q(B^S)\theta_q(B)\epsilon_t$$
Here $\Phi_P$ and $\Theta_Q$ are the coefficients for the seasonal AR and MA components respectively.
$S$ is the period at which the seasonal trend occurs. For $S$ = 12 there is a yearly trend; for $S$ = 3
there is a quarterly trend.

## The expanded ARIMA with exogenous variables -- the ARIMAX model

ARIMAX stands for autoregressive integrated moving average with exogenous variables. An exogenous variable
is a covariate, $x_t$, that influences the observed time series values $y_t$.  ARIMAX can be specified
by considering these $r$ exogenous variables according to the coefficient vector $\beta \in \mathbb{R}^r$:
$$\phi_p(B)(1 - B)^dy_t = \beta^Tx_t\theta_q(B)\epsilon_t$$
Here $x_t \in \mathbb{R}^r$ is the $t$-th vector of exogenous features.

## A SARIMA forecasting example
We can use the `forcast::auto.arima()` function for estimating $p$, $d$, $q$ as well as the seasonal parameters $P$, $D$, and $Q$.

### SARIMA model for a stationary process
Using the `tseries::nino` data, verify that the data is stationary.

1. Convert the time series object to a data.frame/data.table:
```{r}
data(nino)
nino3.4_dt <- RtsaPkg::ts_to_df(ts_obj = nino3.4, col_name = "Data")
str(nino3.4_dt)
```
2. Plot the time series:
```{r,fig.height=8, fig.width=13}
RplotterPkg::create_scatter_plot(
  df = nino3.4_dt,
  aes_x = "DateTime",
  aes_y = "Data",
  title = "Monthly Sea Surface Temperture (deg C) of Nino Region 3.4 SST indices",
  y_title = "Temperature deg C",
  rot_y_tic_label = TRUE,
  x_limits = c(as.Date("1950-01-01"), as.Date("2000-01-01")),
  x_major_breaks = seq(as.Date("1950-01-01"), as.Date("2000-01-01"), "2 year"),
  x_date_labels = "%Y",
  show_pts = FALSE,
  connect = TRUE
)
```

3. It appears that the series is stationary with no trend.  Verify whether there is any seasonal trend by decomposing the series:
```{r, fig.width=14, fig.height=12}
nino3.4_decompose <- RtsaPkg::graph_decompose(
  series_ts = nino3.4,
  title = "Monthly Sea Surface Temperture (deg C) of Nino Region 3.4 SST indices",
  x_title = "Year",
  y_title = "Temperature deg C",
  x_limits = c(as.Date("1950-01-01"), as.Date("2000-01-01")),
  x_major_breaks = seq(as.Date("1950-01-01"), as.Date("2000-01-01"), "2 year"),
  x_date_labels = "%Y",
  col_width = 12
)
```

### Estimates of the seasonal portion of SARIMA model
Specify additional parameters $(P,D,Q)_S$. Since the seasonal trend doesnot dominate the time series data we will set $D = 0$. Also the data is a yearly trend so $S = 12$. To determine the other parameters for the model, consider the plots for the seasonal component from the decompose.

```{r, fig.width=12, fig.height=6}
seasonal_ac <- RtsaPkg::graph_acf(
  df = nino3.4_decompose$decompose_df,
  time_col = "Time",
  value_col = "Seasonal",
  max_lag = 30,
  title = "nino3.4 Seasonal Component",
  confid_level = 1.96,
  show_obs = FALSE,
  show_ac = FALSE,
  show_minor_grids = FALSE,
  bold_y = 0.0,
  row_height = 4,
  col_width = 10
)
```

Use an AR term of order 2 for the seasonal component. Thus $P=2$ and $Q=0$ and the seasonal model is specified by (2,0,0).

### Estimates of the non-seasonal portion of SARIMA model

We need to find $p$ and $q$. For this purpose we will plot the ACF and pACF to identify the values of the MA and AR parameters.

```{r,fig.width=12, fig.height=10}
non_seasonal_ac <- RtsaPkg::graph_acf(
  df = nino3.4_dt,
  time_col = "DateTime",
  value_col = "Data",
  max_lag = 30,
  title = "nino3.4 Non-Seasonal Component",
  confid_level = 1.96,
  show_obs = FALSE,
  show_ac = TRUE,
  show_pc = TRUE,
  ac_y_limits = c(-0.4,1.0),
  ac_y_major_breaks = seq(-0.4,1.0,0.1),
  pac_y_limits = c(-0.4,1.0),
  pac_y_major_breaks = seq(-0.4,1.0,0.1),
  show_minor_grids = FALSE,
  bold_y = 0.0,
  row_height = 3.5,
  col_width = 10
)
```

Set the AR order to 2 and the MA order to 1. This gives the final model: $(2,0,1)x(2,0,0)_{12}$. 

### Fit the model
Fit the model using the `forecast::Arima()` function.

```{r}
order_non_seasonal <- c(2,0,1)
order_seasonal <- c(2,0,0)
nino3.4_arima <- forecast::Arima(nino3.4, order = order_non_seasonal, seasonal = order_seasonal)
```

### Make predictions from the model
There are two ways to obtain predictions from a forecasting model. The first approach relies on the `stats::predict()` function while the second approach uses `forcast::forcast()`.

1. Using `stats::predict()` predict 5 years into the future:
```{r}
forecast_predict <- stats::predict(nino3.4_arima, n.ahead = 60)
```

2. Show time series info for `nino3.4`:
```{r}
TSstudio::ts_info(nino3.4)
```
3. Show time series info for `forecast_predict$pred`:
```{r}
TSstudio::ts_info(forecast_predict$pred)
```
4. Convert `forecast_predict$pred` to a data.frame/data.table:
```{r}
predict_dt <- RtsaPkg::ts_to_df(forecast_predict$pred, col_name = "Data")
str(predict_dt)
```
5. Add a *Source* column to both `nino3.4_dt` with `predict_dt` and row bind them:
```{r}
nino3.4_dt[, Source := "Observation"]
predict_dt[, Source := "Prediction"]
nino3.4_combined_dt <- rbind(nino3.4_dt, predict_dt)
str(nino3.4_combined_dt)
```
6. Plot the combined data.frames:
```{r, fig.width=12, fig.height=8}
RplotterPkg::create_scatter_plot(
  df = nino3.4_combined_dt,
  aes_x = "DateTime",
  aes_y = "Data",
  aes_color = "Source",
  title = "Monthly Predictions of Sea Surface Temperture (deg C) of Nino Region 3.4",
  subtitle = "Nov 1999 to Oct 2004",
  y_title = "Temperture",
  rot_y_tic_label = TRUE,
  x_limits = c(as.Date("1950-01-01"), as.Date("2004-10-01")),
  x_major_breaks = seq(as.Date("1950-01-01"), as.Date("2004-10-01"), "2 year"),
  x_date_labels = "%Y",
  connect = TRUE,
  show_pts = FALSE,
  palette_colors = c("green", "red")
)
```

8. Using the builtin plotting of the `forecast::forcast()`:
```{r,fig.width=12, fig.height=8}
forecast_builtin <- forecast::forecast(nino3.4_arima, h = 60)
plot(forecast_builtin)
```

## ARIMA model for non-stationary data
Will use the `astsa::gtemp` data set.
```{r}
TSstudio::ts_info(astsa::gtemp)
```
### Plot the non-stationary data
```{r, fig.width=12, fig.height=8}
gtemp_dt <- RtsaPkg::ts_to_df(ts_obj = astsa::gtemp)

RplotterPkg::create_scatter_plot(
  df = gtemp_dt,
  aes_x = "DateTime",
  aes_y = "V1",
  title = "Global Yearly Mean Land-Ocean Temperature Deviatioins (deg C)",
  subtitle = "1880 - 2009",
  rot_y_tic_label = TRUE,
  y_title = "Temperture (deg C)",
  x_limits = c(as.Date("1880-01-01"), as.Date("2010-01-01")),
  x_major_breaks = seq(as.Date("1880-01-01"), as.Date("2010-01-01"), "5 year"),
  x_date_labels = "%Y",
  connect = TRUE
)
```

### Make the data stationary
With the values increasing over time it will be necessary to difference the data. To make the data stationary use $d = 1$.

1. Take the first order difference:
```{r}
diff_v <- c(diff(gtemp_dt$V1),NA)
gtemp_dt[, Diff := diff_v]
gtemp_dt <- na.omit(gtemp_dt, cols = "Diff")
str(gtemp_dt)
```

2. Plot the *Diff* column:
```{r, fig.width=12, fig.height=8}
RplotterPkg::create_scatter_plot(
  df = gtemp_dt,
  aes_x = "DateTime",
  aes_y = "Diff",
  title = "Global Yearly Mean Land-Ocean Temperature Deviatioins (deg C)",
  subtitle = "Differences 1880 - 2009",
  rot_y_tic_label = TRUE,
  y_title = "Temperture (deg C)",
  x_limits = c(as.Date("1880-01-01"), as.Date("2010-01-01")),
  x_major_breaks = seq(as.Date("1880-01-01"), as.Date("2010-01-01"), "5 year"),
  x_date_labels = "%Y",
  connect = TRUE
)
```

The trend is removed and because the frequency is 1 year there is no seasonal component. To identify $p$ and $q$ consider the ACF and pACF plots:
```{r, fig.width=12, fig.height=8}
gtemp_diff_ar <- RtsaPkg::graph_acf(
  df = gtemp_dt,
  time_col = "DateTime",
  value_col = "Diff",
  max_lag = 30,
  title = "Global Yearly Mean Land-Ocean Temperature Deviatioins (deg C)",
  subtitle = "Differences 1880 - 2009",
  ac_x_limits = c(1,30),
  ac_x_major_breaks = seq(1,30,1),
  ac_y_limits = c(-0.4,0.4),
  ac_y_major_breaks = seq(-0.4,0.4,0.1),
  pac_y_limits = c(-0.4, 0.4),
  pac_y_major_breaks = seq(-0.4,0.4,0.1),
  confid_level = 1.96,
  show_obs = FALSE,
  show_minor_grids = FALSE,
  bold_y = 0.0,
  row_height = 3,
  col_width = 11
)
```

Since the first lag's autocorrelation is negative use a moving average model. Thus set $p = 0$ and $q = 1$. This leads to
ARIMA(0,1,1) model. Since the data are subject to increasing values, include a drift term in the model to take this effect
into account:

1. Create the *gtemp_arima* model:
```{r}
order_non_seasonal <- c(0,1,1)
gtemp_arima <- stats::arima(astsa::gtemp, order = order_non_seasonal)
summary(gtemp_arima)
```

2. Calculate and view the predictions:
```{r}
gtemp_predict <- stats::predict(gtemp_arima, n.ahead = 30)
```
```{r}
gtemp_forecast_predict <- forecast::forecast(gtemp_arima, h = 30)
plot(gtemp_forecast_predict)
```
